{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99dd5e7e-e9a2-4191-8a50-2603e9f9c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import argparse\n",
    "import importlib.util\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Function to load the model architecture from a given .py file\n",
    "def load_model_class(model_py_path):\n",
    "    spec = importlib.util.spec_from_file_location(\"EEGClassifier\", model_py_path)\n",
    "    model_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(model_module)\n",
    "    return model_module.EEGClassifier\n",
    "\n",
    "# Function to load the saved model\n",
    "def load_model(model_path, model_class, device='cuda'):\n",
    "    model = model_class()  # Initialize the model architecture (EEGClassifier in your case)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Function to load test data from .npy files\n",
    "def load_test_data():\n",
    "    test_data = np.load('data/test/data.npy')\n",
    "    test_labels = np.load('data/test/labels.npy')\n",
    "    return test_data, test_labels\n",
    "\n",
    "# Batched inference to reduce memory usage, assuming model outputs probabilities\n",
    "def batched_inference(model, test_data, batch_size=64, device='cuda'):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(0, len(test_data), batch_size):\n",
    "        batch_data = torch.tensor(test_data[i:i+batch_size]).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_output = model(batch_data).cpu().numpy()  # Move to CPU to free GPU memory\n",
    "        outputs.append(batch_output)\n",
    "\n",
    "    outputs = np.vstack(outputs)  # Combine batches into one array\n",
    "    return outputs\n",
    "\n",
    "# Function to evaluate metrics (precision, recall, F1 score, support, etc.)\n",
    "def evaluate_metrics(test_labels, predictions, outputs, num_classes):\n",
    "    precision = precision_score(test_labels, predictions, average='weighted')\n",
    "    recall = recall_score(test_labels, predictions, average='weighted')\n",
    "    f1 = f1_score(test_labels, predictions, average='weighted')\n",
    "    kappa = cohen_kappa_score(test_labels, predictions)\n",
    "\n",
    "    # Calculate AUC-ROC (multi-class)\n",
    "    try:\n",
    "        # One-hot encode the labels for multi-class ROC-AUC calculation\n",
    "        test_labels_bin = label_binarize(test_labels, classes=list(range(num_classes)))\n",
    "        auc_score = roc_auc_score(test_labels_bin, outputs, multi_class='ovo', average='weighted')\n",
    "    except ValueError:\n",
    "        auc_score = None\n",
    "        print(\"AUC-ROC could not be calculated due to data formatting.\")\n",
    "    \n",
    "    # Generate support for each class from classification report\n",
    "    report_dict = classification_report(test_labels, predictions, output_dict=True)\n",
    "    support = {label: metrics[\"support\"] for label, metrics in report_dict.items() if label.isdigit()}\n",
    "\n",
    "    return precision, recall, f1, kappa, auc_score, support\n",
    "\n",
    "# Function to plot ROC curve for each class and save\n",
    "def plot_roc_curve(test_labels, outputs, num_classes, model_name):\n",
    "    test_labels_bin = label_binarize(test_labels, classes=list(range(num_classes)))\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(test_labels_bin[:, i], outputs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    roc_filename = f\"./output/models/{model_name}/{model_name}_roc_curve.png\"\n",
    "    plt.savefig(roc_filename)\n",
    "    print(f\"Saved ROC curve as {roc_filename}\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot Precision-Recall curve for each class and save\n",
    "def plot_pr_curve(test_labels, outputs, num_classes, model_name):\n",
    "    test_labels_bin = label_binarize(test_labels, classes=list(range(num_classes)))\n",
    "\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    pr_auc = dict()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(test_labels_bin[:, i], outputs[:, i])\n",
    "        pr_auc[i] = auc(recall[i], precision[i])\n",
    "        plt.plot(recall[i], precision[i], label=f'Class {i} (AUC = {pr_auc[i]:.2f})')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc='lower left')\n",
    "    pr_filename = f\"./output/models/{model_name}/{model_name}_pr_curve.png\"\n",
    "    plt.savefig(pr_filename)\n",
    "    print(f\"Saved Precision-Recall curve as {pr_filename}\")\n",
    "    plt.show()\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "def generate_confusion_matrix(test_labels, predictions, model_name):\n",
    "    conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "    report = classification_report(test_labels, predictions)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    conf_matrix_filename = f\"./output/models/{model_name}/{model_name}_confusion_matrix.png\"\n",
    "    plt.savefig(conf_matrix_filename)\n",
    "    print(f\"Saved Confusion Matrix as {conf_matrix_filename}\")\n",
    "    plt.show()\n",
    "\n",
    "    return conf_matrix, report\n",
    "\n",
    "# Function to save evaluation metrics to a file\n",
    "def save_results_to_file(precision, recall, f1, kappa, auc_score, support, conf_matrix, report, test_duration, model_name):\n",
    "    file_name = f\"./output/models/{model_name}/{model_name}_eval_result.txt\"\n",
    "    with open(file_name, \"w\") as f:\n",
    "        f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.4f}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"Kappa: {kappa:.4f}\\n\")\n",
    "        f.write(f\"AUC: {auc_score:.4f}\\n\" if auc_score else \"AUC: Not calculated\\n\")\n",
    "        f.write(f\"Test Duration: {test_duration:.4f} seconds\\n\")\n",
    "        f.write(f\"Support (number of true instances per class):\\n\")\n",
    "        for label, count in support.items():\n",
    "            f.write(f\"Class {label}: {count} instances\\n\")\n",
    "        f.write(f\"Confusion Matrix:\\n{conf_matrix}\\n\")\n",
    "        f.write(f\"Classification Report:\\n{report}\\n\")\n",
    "    print(f\"Saved evaluation results as {file_name}\")\n",
    "\n",
    "# Main evaluation function\n",
    "def evaluate_model_with_timing(model, test_data, test_labels, num_classes=6, batch_size=120, device='cuda', model_name=\"model\"):\n",
    "    # Start time for inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run batched inference\n",
    "    outputs = batched_inference(model, test_data, batch_size=batch_size, device=device)\n",
    "\n",
    "    # End time for inference\n",
    "    end_time = time.time()\n",
    "    test_duration = end_time - start_time\n",
    "\n",
    "    # Predictions and ground truth\n",
    "    predictions = np.argmax(outputs, axis=1)\n",
    "\n",
    "    # Evaluate metrics (including support)\n",
    "    precision, recall, f1, kappa, auc_score, support = evaluate_metrics(test_labels, predictions, outputs, num_classes)\n",
    "    \n",
    "    # Confusion matrix and classification report\n",
    "    conf_matrix, report = generate_confusion_matrix(test_labels, predictions, model_name)\n",
    "\n",
    "    # Plot ROC and PR curves\n",
    "    plot_roc_curve(test_labels, outputs, num_classes, model_name)\n",
    "    plot_pr_curve(test_labels, outputs, num_classes, model_name)\n",
    "\n",
    "    return precision, recall, f1, kappa, auc_score, support, conf_matrix, report, test_duration\n",
    "\n",
    "# # Main function to load model, test data, and evaluate\n",
    "# def main():\n",
    "def evalfun(model_py_path, model_pt_path):\n",
    "\n",
    "    # parser = argparse.ArgumentParser(description=\"Evaluate a trained model on test data.\")\n",
    "    # parser.add_argument('model_path', type=str, help='Path to the .pt file of the trained model')\n",
    "    # parser.add_argument('model_py_path', type=str, help='Path to the .py file of the model architecture')\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    # Extract model name from the .py file path (remove .py extension)\n",
    "    # model_name = os.path.basename(args.model_py_path).replace('.py', '')\n",
    "    model_name = os.path.basename(model_py_path).replace('.py', '')\n",
    "\n",
    "    # Load model architecture from the provided .py file\n",
    "    # model_class = load_model_class(args.model_py_path)\n",
    "    model_class = load_model_class(model_py_path)\n",
    "\n",
    "    # Load the model\n",
    "    # model = load_model(args.model_path, model_class, device='cuda')\n",
    "    model = load_model(model_pt_path, model_class, device='cuda')\n",
    "\n",
    "    # Load test data\n",
    "    test_data, test_labels = load_test_data()\n",
    "\n",
    "    # Evaluate model and compute metrics with timing\n",
    "    precision, recall, f1, kappa, auc_score, support, conf_matrix, report, test_duration = evaluate_model_with_timing(\n",
    "        model, test_data, test_labels, num_classes=6, batch_size=64, device='cuda', model_name=model_name\n",
    "    )\n",
    "\n",
    "    # Print the metrics to the console\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Kappa: {kappa:.4f}\")\n",
    "    print(f\"AUC: {auc_score:.4f}\" if auc_score else \"AUC: Not calculated\")\n",
    "    print(f\"Support: {support}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    print(f\"Classification Report:\\n{report}\")\n",
    "\n",
    "    # Save the metrics to a file\n",
    "    save_results_to_file(precision, recall, f1, kappa, auc_score, support, conf_matrix, report, test_duration, model_name)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f123a858-2d16-4bf9-a941-b8535b3adb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "========================================\n",
      "Starting pipeline at 2024-10-03 01:22:53.697144\n",
      "========================================\n",
      "\n",
      "\n",
      "Running model: htnet\n",
      "Running model_runner.py with argument: ./modelpy/htnet.py\n",
      "Running: python3 ./model_runner.py ./modelpy/htnet.py\n",
      "Error running ./model_runner.py:\n",
      "\n",
      "  0%|          | 0/532 [00:00<?, ?it/s]\n",
      "  0%|          | 0/532 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/5cd1329d-6ad0-4c91-904e-ba21c872357b/Deepak/Students/Susmit_23CS60R75/model_final/with_sub/ipynbRun/./model_runner.py\", line 727, in <module>\n",
      "    run_and_save_model(args, model_file, source_data, source_labels, target_data, target_labels, test_data,test_labels, mixup_data=None, supervised_mixup_data=supervised_mixup_data)\n",
      "  File \"/mnt/5cd1329d-6ad0-4c91-904e-ba21c872357b/Deepak/Students/Susmit_23CS60R75/model_final/with_sub/ipynbRun/./model_runner.py\", line 552, in run_and_save_model\n",
      "    supervised_run(args, model, source_data, source_labels, target_data, target_labels, test_data, mixup_data=mixup_data, supervised_mixup_data=supervised_mixup_data, filename =  meta_file_path)\n",
      "  File \"/mnt/5cd1329d-6ad0-4c91-904e-ba21c872357b/Deepak/Students/Susmit_23CS60R75/model_final/with_sub/ipynbRun/./model_runner.py\", line 449, in supervised_run\n",
      "    train_model(args, model,\n",
      "  File \"/mnt/5cd1329d-6ad0-4c91-904e-ba21c872357b/Deepak/Students/Susmit_23CS60R75/model_final/with_sub/ipynbRun/./model_runner.py\", line 345, in train_model\n",
      "    _, _, train_score, train_loss = train_epoch(args, model, train_loader, criterion, optimizer, scheduler, epoch)\n",
      "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/5cd1329d-6ad0-4c91-904e-ba21c872357b/Deepak/Students/Susmit_23CS60R75/model_final/with_sub/ipynbRun/./model_runner.py\", line 215, in train_epoch\n",
      "    lds = vat_loss(model, eeg)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/deepak/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/deepak/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/5cd1329d-6ad0-4c91-904e-ba21c872357b/Deepak/Students/Susmit_23CS60R75/model_final/with_sub/ipynbRun/./model_runner.py\", line 174, in forward\n",
      "    pred = F.softmax(model(x), dim=1)\n",
      "                     ^^^^^^^^\n",
      "  File \"/home/deepak/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/deepak/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/5cd1329d-6ad0-4c91-904e-ba21c872357b/Deepak/Students/Susmit_23CS60R75/model_final/with_sub/ipynbRun/modelpy/htnet.py\", line 79, in forward\n",
      "    x = torch.relu(self.fc1(x))\n",
      "                   ^^^^^^^^^^^\n",
      "  File \"/home/deepak/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/deepak/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/deepak/miniconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py\", line 117, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: mat1 and mat2 shapes cannot be multiplied (256x1 and 256x256)\n",
      "\n",
      "Running evaluatorhtnet\n",
      "EEGClassifier(\n",
      "  (base): BaseEEGClassifier(\n",
      "    (conv1): Conv2d(120, 64, kernel_size=(2, 4), stride=(1, 1), padding=(1, 2))\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(2, 4), stride=(1, 2), padding=(1, 1))\n",
      "    (pool1): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (hilbert): HilbertLayer()\n",
      "    (conv3): Conv2d(64, 128, kernel_size=(2, 4), stride=(1, 1), padding=(1, 2))\n",
      "    (conv4): Conv2d(128, 128, kernel_size=(2, 4), stride=(1, 2), padding=(1, 1))\n",
      "    (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv5): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "    (conv6): Conv2d(256, 256, kernel_size=(4, 4), stride=(1, 2), padding=(1, 1))\n",
      "    (pool3): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (global_avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "  )\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (dropout2): Dropout(p=0.25, inplace=False)\n",
      "  (fc4): Linear(in_features=64, out_features=6, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1787316/3720422732.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './output/models/htnet/htnet_final_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m     log(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPipeline completed at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 68\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Students/Susmit_23CS60R75/model_final/FINAL_run/ipynbRun/output/models/SleepStagerBlanco2020/SleepStagerBlanco2020_final_model.pt\u001b[39;00m\n\u001b[1;32m     64\u001b[0m         \n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m         \u001b[38;5;66;03m# Step 4: Run evaluator.py with model.pt and model.py as arguments\u001b[39;00m\n\u001b[1;32m     67\u001b[0m         log(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning evaluator\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m         \u001b[43mevalfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_py_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_pt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;66;03m# run_script(evaluator_script, model_pt_path, model_py_path)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     log(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPipeline completed at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 204\u001b[0m, in \u001b[0;36mevalfun\u001b[0;34m(model_py_path, model_pt_path)\u001b[0m\n\u001b[1;32m    200\u001b[0m model_class \u001b[38;5;241m=\u001b[39m load_model_class(model_py_path)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# model = load_model(args.model_path, model_class, device='cuda')\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_pt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# Load test data\u001b[39;00m\n\u001b[1;32m    207\u001b[0m test_data, test_labels \u001b[38;5;241m=\u001b[39m load_test_data()\n",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_path, model_class, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(model_path, model_class, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     24\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_class()  \u001b[38;5;66;03m# Initialize the model architecture (EEGClassifier in your case)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     26\u001b[0m     model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     27\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './output/models/htnet/htnet_final_model.pt'"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available and print whether using CUDA or CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define paths to scripts\n",
    "dataloader_script = './dataloader.py'\n",
    "model_runner_script = './model_runner.py'\n",
    "evaluator_script = './evaluator.py'\n",
    "\n",
    "# Log file path\n",
    "log_dir = './output/'\n",
    "log_file = os.path.join(log_dir, 'pipeline_log.txt')\n",
    "\n",
    "# Ensure the log directory exists\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Function to log both to the console and to a file\n",
    "def log(message):\n",
    "    print(message)\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"{message}\\n\")\n",
    "\n",
    "# Function to run a Python script and log the output\n",
    "def run_script(script_path, *args):\n",
    "    command = ['python3', script_path] + list(args)\n",
    "    try:\n",
    "        log(f\"Running: {' '.join(command)}\")\n",
    "        result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        log(result.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        log(f\"Error running {script_path}:\")\n",
    "        log(e.stderr)\n",
    "\n",
    "def main():\n",
    "    # Start logging\n",
    "    log(f\"\\n{'='*40}\\nStarting pipeline at {datetime.now()}\\n{'='*40}\\n\")\n",
    "\n",
    "    # Step 1: Run dataloader.py to load data\n",
    "    # log(\"Running dataloader.py...\")\n",
    "    # run_script(dataloader_script)\n",
    "\n",
    "    # Step 2: Iterate over each model in the modelpy/ directory\n",
    "    model_files = glob.glob('./modelpy/*.py')  # List all .py files in modelpy/\n",
    "\n",
    "    for model_py_path in model_files:\n",
    "        # Extract the base model name (without .py extension)\n",
    "        model_name = os.path.basename(model_py_path).replace('.py', '')\n",
    "\n",
    "        log(f\"\\nRunning model: {model_name}\")\n",
    "\n",
    "        # Step 3: Run model_runner.py with model.py as argument\n",
    "        log(f\"Running model_runner.py with argument: {model_py_path}\")\n",
    "        run_script(model_runner_script, model_py_path)\n",
    "        \n",
    "        # Path for the corresponding .pt file (adjust this according to how your models are saved)\n",
    "        model_pt_path = f'./output/models/{model_name}/{model_name}_final_model.pt'\n",
    "# Students/Susmit_23CS60R75/model_final/FINAL_run/ipynbRun/output/models/SleepStagerBlanco2020/SleepStagerBlanco2020_final_model.pt\n",
    "        \n",
    "\n",
    "        # Step 4: Run evaluator.py with model.pt and model.py as arguments\n",
    "        log(f\"Running evaluator{model_name}\")\n",
    "        evalfun(model_py_path, model_pt_path)\n",
    "        # run_script(evaluator_script, model_pt_path, model_py_path)\n",
    "\n",
    "    log(f\"\\nPipeline completed at {datetime.now()}\\n{'='*40}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab91f6-52d7-4e7f-a18f-bac7d9903ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
