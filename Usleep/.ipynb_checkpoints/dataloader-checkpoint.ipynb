{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33a169e9-d973-4479-8029-d14b3fb07081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72c9c16b-bdfb-4dff-ac43-70b019fb63e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER ='/home/deepak/Documents/Deepak/Students/Susmit_23CS60R75/Sleep_Data/'\n",
    "SOURCE_DATA_FOLDER = DATA_FOLDER  + \"SleepSource/\"\n",
    "LEADERBOARD_TARGET_DATA_FOLDER = DATA_FOLDER + \"LeaderboardSleep/sleep_target/\"\n",
    "LEADERBOARD_TEST_DATA_FOLDER = DATA_FOLDER + \"LeaderboardSleep/testing/\"\n",
    "FINAL_TARGET_DATA_FOLDER = DATA_FOLDER + \"finalSleep/sleep_target/\"\n",
    "FINAL_TEST_DATA_FOLDER = DATA_FOLDER + \"finalSleep/testing/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c91f36e0-b88d-4602-889e-86db7a7e76e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_folder, load_labels=True):\n",
    "    fn_list = sorted(os.listdir(data_folder))\n",
    "    print(f\"Loading data from folder: {data_folder} ({len(fn_list)} files) - Load labels {load_labels}\")\n",
    "\n",
    "    data_map = {}\n",
    "    subject_list = []\n",
    "    sample_counter = 0\n",
    "\n",
    "    for fn in tqdm(fn_list):\n",
    "        if fn.endswith(\"X.npy\"):\n",
    "            code = fn.split(\"_\")[1][:-4]\n",
    "        elif fn == \"headerInfo.npy\":\n",
    "            meta = np.load(data_folder + fn, allow_pickle=True)\n",
    "            print(meta)\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        eeg = np.load(data_folder + fn, allow_pickle=True)\n",
    "\n",
    "        if load_labels:\n",
    "            label_fn = fn.replace(\"X\", \"y\")\n",
    "            label = np.load(data_folder + label_fn, allow_pickle=True)\n",
    "        else:\n",
    "            label = None\n",
    "\n",
    "        s_part, r_part = code.split(\"r\")\n",
    "        subject = int(s_part[1:])\n",
    "        repetition = int(r_part[:-1])\n",
    "        \n",
    "        subject_list.append(subject)\n",
    "\n",
    "        if subject not in data_map.keys():\n",
    "            data_map[subject] = {}\n",
    "\n",
    "        data_map[subject][repetition] = {\"eeg\": eeg, \"label\": label}\n",
    "        sample_counter += len(eeg)\n",
    "\n",
    "    subject_list = np.unique(subject_list)\n",
    "    print(f\"Loaded total {sample_counter} samples for subjects: {subject_list}\")\n",
    "    return data_map, subject_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f4db10f-641b-4fe2-b2fc-64f23d6b7b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_window_data(data, subject_list=None):\n",
    "    window_data = []\n",
    "    window_labels = []\n",
    "\n",
    "    if subject_list is None:\n",
    "        subject_list = data.keys()\n",
    "\n",
    "    for s in tqdm(subject_list):\n",
    "        for r in data[s].keys():\n",
    "            eeg = data[s][r][\"eeg\"]\n",
    "            label = data[s][r][\"label\"]\n",
    "\n",
    "            window_data.extend(eeg)\n",
    "            \n",
    "            if label is not None:\n",
    "                window_labels.extend(label)\n",
    "\n",
    "    return window_data, window_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c42d126c-fe74-486d-a3f1-9dec7ea14da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(desc, data):\n",
    "    print(f\"{desc} mean: {np.mean(data)}, std: {np.std(data)}, min: {np.min(data)}, max: {np.max(data)}\")\n",
    "\n",
    "def normalize(data, mean_value, std_value, desc=\"\"):\n",
    "    data = np.array(data)\n",
    "    data = (data - mean_value) / std_value\n",
    "    print_stats(desc, data)\n",
    "    return list(data)\n",
    "\n",
    "def filter_freq(data, f_min, f_max, FS):\n",
    "    return mne.filter.filter_data(np.array(data, dtype=np.float64), FS, f_min, f_max, method=\"iir\", verbose=False)\n",
    "\n",
    "def downsample(data, FS, FS_new):\n",
    "    return mne.filter.resample(data, down=FS/FS_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c23de2e-882a-4cdb-b2e0-4af02cb6a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def seed_everything(seed):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "191af9f2-f10a-4e2a-b3cf-e60841045d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase_1_data():\n",
    "    source_data, source_subjects = load_data(SOURCE_DATA_FOLDER)\n",
    "    source_data, source_labels = prepare_window_data(source_data, source_subjects)\n",
    "    # calculate stats of source data and normalize it\n",
    "    source_data = np.array(source_data)\n",
    "    source_mean = np.mean(source_data)\n",
    "    source_std = np.std(source_data)\n",
    "    source_data = (source_data - source_mean) / source_std\n",
    "    print(f\"Source mean: {source_mean}, std: {source_std}, min: {np.min(source_data)}, max: {np.max(source_data)}\")\n",
    "    source_data  = list(source_data)\n",
    "    # load and normalize target data\n",
    "    lb_target_data, lb_target_subjects = load_data(LEADERBOARD_TARGET_DATA_FOLDER)\n",
    "    lb_target_data, lb_target_labels = prepare_window_data(lb_target_data, lb_target_subjects)\n",
    "    lb_target_data = normalize(lb_target_data, source_mean, source_std, \"Leadeboard target\")\n",
    "    # load and normalize test data\n",
    "    lb_test_data, lb_test_subjects = load_data(LEADERBOARD_TEST_DATA_FOLDER, load_labels=False)\n",
    "    lb_test_data, lb_test_labels = prepare_window_data(lb_test_data, lb_test_subjects)\n",
    "    lb_test_data = normalize(lb_test_data, source_mean, source_std, \"Leadeboard test\")\n",
    "\n",
    "    return source_data, source_labels, lb_target_data, lb_target_labels, lb_test_data, lb_test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d82ee556-1012-4ac7-a0ac-aa19ab72522f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from folder: /home/deepak/Documents/Deepak/Students/Susmit_23CS60R75/Sleep_Data/SleepSource/ (158 files) - Load labels True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/158 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Info | 8 non-empty values\n",
      " bads: []\n",
      " ch_names: Fpz-Cz, Pz-Oz\n",
      " chs: 2 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.5 Hz\n",
      " lowpass: 100.0 Hz\n",
      " meas_date: 1991-09-26 15:00:00 UTC\n",
      " nchan: 2\n",
      " projs: []\n",
      " sfreq: 100.0 Hz\n",
      " subject_info: 2 items (dict)\n",
      ">\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 158/158 [00:38<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded total 90545 samples for subjects: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 39/39 [00:00<00:00, 1965.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source mean: -2.175762164676705e-07, std: 1.6836217270029333e-05, min: -14.7172265485441, max: 12.426637935405019\n",
      "Loading data from folder: /home/deepak/Documents/Deepak/Students/Susmit_23CS60R75/Sleep_Data/LeaderboardSleep/sleep_target/ (25 files) - Load labels True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Info | 8 non-empty values\n",
      " bads: []\n",
      " ch_names: Fpz-Cz, Pz-Oz\n",
      " chs: 2 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.5 Hz\n",
      " lowpass: 100.0 Hz\n",
      " meas_date: 1990-03-13 15:09:00 UTC\n",
      " nchan: 2\n",
      " projs: []\n",
      " sfreq: 100.0 Hz\n",
      " subject_info: 2 items (dict)\n",
      ">\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:06<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded total 15442 samples for subjects: [0 1 2 3 4 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 6/6 [00:00<00:00, 2661.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leadeboard target mean: 0.038902552623313796, std: 1.0560441229752748, min: -11.688042547053058, max: 12.010867582259191\n",
      "Loading data from folder: /home/deepak/Documents/Deepak/Students/Susmit_23CS60R75/Sleep_Data/LeaderboardSleep/testing/ (25 files) - Load labels False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:10<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded total 25748 samples for subjects: [ 6  7  8  9 10 11 12 13 14 15 16 17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 12/12 [00:00<00:00, 3713.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leadeboard test mean: 0.014119353426048422, std: 0.8645048345254686, min: -11.985021370728651, max: 11.832680288053835\n"
     ]
    }
   ],
   "source": [
    "source_data, source_labels, target_data, target_labels, test_data, mixup_data = get_phase_1_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ade8c04-264b-4bc8-957f-8cb9e27266bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (6010, 2, 3000)\n",
      "1 (1672, 2, 3000)\n",
      "2 (5035, 2, 3000)\n",
      "3 (704, 2, 3000)\n",
      "4 (414, 2, 3000)\n",
      "5 (1607, 2, 3000)\n"
     ]
    }
   ],
   "source": [
    "tmp = np.array(target_data)\n",
    "supervised_mixup_data = {}\n",
    "for c in np.unique(target_labels):\n",
    "    supervised_mixup_data[c] = tmp[target_labels == c]\n",
    "    print(c, np.shape(supervised_mixup_data[c]))\n",
    "\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df0e45d2-3731-4f98-a619-6668bc2ba0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created: data/train\n",
      "Directory created: data/validate\n",
      "Directory created: data/test\n",
      "Saved train data to data/train/data.npy and labels to data/train/labels.npy\n",
      "Saved validate data to data/validate/data.npy and labels to data/validate/labels.npy\n",
      "Saved test data to data/test/data.npy and labels to data/test/labels.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def split_train_val_test(train_data, train_labels, train_size=0.8, val_size=0.1, test_size=0.1, random_state=42):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # First, split into training and temp (validation + test)\n",
    "    train_data_split, temp_data, train_labels_split, temp_labels = train_test_split(\n",
    "        train_data, train_labels, train_size=train_size, random_state=random_state, stratify=train_labels\n",
    "    )\n",
    "\n",
    "    # Calculate the proportion of validation and test sizes relative to temp_data\n",
    "    temp_val_size = val_size / (val_size + test_size)\n",
    "    val_data, test_data, val_labels, test_labels = train_test_split(\n",
    "        temp_data, temp_labels, test_size=1 - temp_val_size, random_state=random_state, stratify=temp_labels\n",
    "    )\n",
    "\n",
    "    return train_data_split, val_data, test_data, train_labels_split, val_labels, test_labels\n",
    "\n",
    "def create_data_directories(base_dir='data', subdirs=['train', 'validate', 'test']):\n",
    "    for subdir in subdirs:\n",
    "        path = os.path.join(base_dir, subdir)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        print(f\"Directory created: {path}\")\n",
    "\n",
    "def save_splits(base_dir, splits):\n",
    "    \"\"\"\n",
    "    Saves the data and labels into respective directories.\n",
    "\n",
    "    Parameters:\n",
    "    - base_dir: The main directory containing subdirectories.\n",
    "    - splits: A tuple containing (train_data, val_data, test_data, train_labels, val_labels, test_labels).\n",
    "    \"\"\"\n",
    "    train_data, val_data, test_data, train_labels, val_labels, test_labels = splits\n",
    "    splits_dict = {\n",
    "        'train': (train_data, train_labels),\n",
    "        'validate': (val_data, val_labels),\n",
    "        'test': (test_data, test_labels)\n",
    "    }\n",
    "\n",
    "    for split_name, (data, labels) in splits_dict.items():\n",
    "        data_path = os.path.join(base_dir, split_name, 'data.npy')\n",
    "        labels_path = os.path.join(base_dir, split_name, 'labels.npy')\n",
    "        np.save(data_path, data)\n",
    "        np.save(labels_path, labels)\n",
    "        print(f\"Saved {split_name} data to {data_path} and labels to {labels_path}\")\n",
    "\n",
    "\n",
    "splits = split_train_val_test(source_data, source_labels)\n",
    "create_data_directories()\n",
    "save_splits('data', splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ed454d1-f577-4442-960e-b85788b1c2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Load train data and labels\n",
    "# source_data = np.load('data/train/data.npy')\n",
    "# source_labels = np.load('data/train/labels.npy')\n",
    "\n",
    "# # Load validate data and labels\n",
    "# target_data = np.load('data/validate/data.npy')\n",
    "# target_labels = np.load('data/validate/labels.npy')\n",
    "\n",
    "# # Load test data and labels\n",
    "# test_data = np.load('data/test/data.npy')\n",
    "# test_labels = np.load('data/test/labels.npy')\n",
    "\n",
    "# # Print the shape of each\n",
    "# print(\"Train data shape:\", source_data.shape)\n",
    "# print(\"Train labels shape:\", source_labels.shape)\n",
    "# print(\"Validate data shape:\", target_data.shape)\n",
    "# print(\"Validate labels shape:\", target_labels.shape)\n",
    "# print(\"Test data shape:\", test_data.shape)\n",
    "# print(\"Test labels shape:\", test_labels.shape)\n",
    "\n",
    "# # Calculate and print total data size (total number of samples)\n",
    "# total_data_size = source_labels.shape[0] + target_data.shape[0] + test_data.shape[0]\n",
    "# print(\"Total data size (number of samples):\", total_data_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
