{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61c49037-b2a6-43e1-ae8b-0755cfd3c9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] model_path model_py_path\n",
      "ipykernel_launcher.py: error: the following arguments are required: model_py_path\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import argparse\n",
    "import importlib.util\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Function to load the model architecture from a given .py file\n",
    "def load_model_class(model_py_path):\n",
    "    spec = importlib.util.spec_from_file_location(\"EEGClassifier\", model_py_path)\n",
    "    model_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(model_module)\n",
    "    return model_module.EEGClassifier\n",
    "\n",
    "# Function to load the saved model\n",
    "def load_model(model_path, model_class, device='cuda'):\n",
    "    model = model_class()  # Initialize the model architecture (EEGClassifier in your case)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Function to load test data from .npy files\n",
    "def load_test_data():\n",
    "    test_data = np.load('data/test/data.npy')\n",
    "    test_labels = np.load('data/test/labels.npy')\n",
    "    return test_data, test_labels\n",
    "\n",
    "# Batched inference to reduce memory usage, assuming model outputs probabilities\n",
    "def batched_inference(model, test_data, batch_size=64, device='cuda'):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(0, len(test_data), batch_size):\n",
    "        batch_data = torch.tensor(test_data[i:i+batch_size]).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_output = model(batch_data).cpu().numpy()  # Move to CPU to free GPU memory\n",
    "        outputs.append(batch_output)\n",
    "\n",
    "    outputs = np.vstack(outputs)  # Combine batches into one array\n",
    "    return outputs\n",
    "\n",
    "# Function to evaluate metrics (precision, recall, F1 score, support, etc.)\n",
    "def evaluate_metrics(test_labels, predictions, outputs, num_classes):\n",
    "    precision = precision_score(test_labels, predictions, average='weighted')\n",
    "    recall = recall_score(test_labels, predictions, average='weighted')\n",
    "    f1 = f1_score(test_labels, predictions, average='weighted')\n",
    "    kappa = cohen_kappa_score(test_labels, predictions)\n",
    "\n",
    "    # Calculate AUC-ROC (multi-class)\n",
    "    try:\n",
    "        # One-hot encode the labels for multi-class ROC-AUC calculation\n",
    "        test_labels_bin = label_binarize(test_labels, classes=list(range(num_classes)))\n",
    "        auc_score = roc_auc_score(test_labels_bin, outputs, multi_class='ovo', average='weighted')\n",
    "    except ValueError:\n",
    "        auc_score = None\n",
    "        print(\"AUC-ROC could not be calculated due to data formatting.\")\n",
    "    \n",
    "    # Generate support for each class from classification report\n",
    "    report_dict = classification_report(test_labels, predictions, output_dict=True)\n",
    "    support = {label: metrics[\"support\"] for label, metrics in report_dict.items() if label.isdigit()}\n",
    "\n",
    "    return precision, recall, f1, kappa, auc_score, support\n",
    "\n",
    "# Function to plot ROC curve for each class and save\n",
    "def plot_roc_curve(test_labels, outputs, num_classes, model_name):\n",
    "    test_labels_bin = label_binarize(test_labels, classes=list(range(num_classes)))\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(test_labels_bin[:, i], outputs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    roc_filename = f\"./output/{model_name}/{model_name}_roc_curve.png\"\n",
    "    plt.savefig(roc_filename)\n",
    "    print(f\"Saved ROC curve as {roc_filename}\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot Precision-Recall curve for each class and save\n",
    "def plot_pr_curve(test_labels, outputs, num_classes, model_name):\n",
    "    test_labels_bin = label_binarize(test_labels, classes=list(range(num_classes)))\n",
    "\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    pr_auc = dict()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(test_labels_bin[:, i], outputs[:, i])\n",
    "        pr_auc[i] = auc(recall[i], precision[i])\n",
    "        plt.plot(recall[i], precision[i], label=f'Class {i} (AUC = {pr_auc[i]:.2f})')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc='lower left')\n",
    "    pr_filename = f\"./output/{model_name}/{model_name}_pr_curve.png\"\n",
    "    plt.savefig(pr_filename)\n",
    "    print(f\"Saved Precision-Recall curve as {pr_filename}\")\n",
    "    plt.show()\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "def generate_confusion_matrix(test_labels, predictions, model_name):\n",
    "    conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "    report = classification_report(test_labels, predictions)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    conf_matrix_filename = f\"./output/{model_name}/{model_name}_confusion_matrix.png\"\n",
    "    plt.savefig(conf_matrix_filename)\n",
    "    print(f\"Saved Confusion Matrix as {conf_matrix_filename}\")\n",
    "    plt.show()\n",
    "\n",
    "    return conf_matrix, report\n",
    "\n",
    "# Function to save evaluation metrics to a file\n",
    "def save_results_to_file(precision, recall, f1, kappa, auc_score, support, conf_matrix, report, test_duration, model_name):\n",
    "    file_name = f\"./output/{model_name}/{model_name}_eval_result.txt\"\n",
    "    with open(file_name, \"w\") as f:\n",
    "        f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.4f}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"Kappa: {kappa:.4f}\\n\")\n",
    "        f.write(f\"AUC: {auc_score:.4f}\\n\" if auc_score else \"AUC: Not calculated\\n\")\n",
    "        f.write(f\"Test Duration: {test_duration:.4f} seconds\\n\")\n",
    "        f.write(f\"Support (number of true instances per class):\\n\")\n",
    "        for label, count in support.items():\n",
    "            f.write(f\"Class {label}: {count} instances\\n\")\n",
    "        f.write(f\"Confusion Matrix:\\n{conf_matrix}\\n\")\n",
    "        f.write(f\"Classification Report:\\n{report}\\n\")\n",
    "    print(f\"Saved evaluation results as {file_name}\")\n",
    "\n",
    "# Main evaluation function\n",
    "def evaluate_model_with_timing(model, test_data, test_labels, num_classes=6, batch_size=120, device='cuda', model_name=\"model\"):\n",
    "    # Start time for inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run batched inference\n",
    "    outputs = batched_inference(model, test_data, batch_size=batch_size, device=device)\n",
    "\n",
    "    # End time for inference\n",
    "    end_time = time.time()\n",
    "    test_duration = end_time - start_time\n",
    "\n",
    "    # Predictions and ground truth\n",
    "    predictions = np.argmax(outputs, axis=1)\n",
    "\n",
    "    # Evaluate metrics (including support)\n",
    "    precision, recall, f1, kappa, auc_score, support = evaluate_metrics(test_labels, predictions, outputs, num_classes)\n",
    "    \n",
    "    # Confusion matrix and classification report\n",
    "    conf_matrix, report = generate_confusion_matrix(test_labels, predictions, model_name)\n",
    "\n",
    "    # Plot ROC and PR curves\n",
    "    plot_roc_curve(test_labels, outputs, num_classes, model_name)\n",
    "    plot_pr_curve(test_labels, outputs, num_classes, model_name)\n",
    "\n",
    "    return precision, recall, f1, kappa, auc_score, support, conf_matrix, report, test_duration\n",
    "\n",
    "# # Main function to load model, test data, and evaluate\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Evaluate a trained model on test data.\")\n",
    "    parser.add_argument('model_path', type=str, help='Path to the .pt file of the trained model')\n",
    "    parser.add_argument('model_py_path', type=str, help='Path to the .py file of the model architecture')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Extract model name from the .py file path (remove .py extension)\n",
    "    model_name = os.path.basename(args.model_py_path).replace('.py', '')\n",
    "\n",
    "    # Load model architecture from the provided .py file\n",
    "    model_class = load_model_class(args.model_py_path)\n",
    "\n",
    "    # Load the model\n",
    "    model = load_model(args.model_path, model_class, device='cuda')\n",
    "\n",
    "    # Load test data\n",
    "    test_data, test_labels = load_test_data()\n",
    "\n",
    "    # Evaluate model and compute metrics with timing\n",
    "    precision, recall, f1, kappa, auc_score, support, conf_matrix, report, test_duration = evaluate_model_with_timing(\n",
    "        model, test_data, test_labels, num_classes=6, batch_size=64, device='cuda', model_name=model_name\n",
    "    )\n",
    "\n",
    "    # Print the metrics to the console\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Kappa: {kappa:.4f}\")\n",
    "    print(f\"AUC: {auc_score:.4f}\" if auc_score else \"AUC: Not calculated\")\n",
    "    print(f\"Support: {support}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    print(f\"Classification Report:\\n{report}\")\n",
    "\n",
    "    # Save the metrics to a file\n",
    "    save_results_to_file(precision, recall, f1, kappa, auc_score, support, conf_matrix, report, test_duration, model_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726faaf1-ce34-4a5e-ab4c-528a8cf29736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
